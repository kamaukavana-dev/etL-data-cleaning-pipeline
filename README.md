ğŸš€ Project Overview

This project is a configurable data quality pipeline designed to clean, validate, analyze, and report on CSV/Excel datasets at scale.

It automatically:

Cleans invalid records (emails, phone numbers, dates, numeric fields)

Tracks drop rates and data quality metrics

Generates structured Excel reports

Sends email notifications and alerts based on thresholds

Runs locally, as a standalone executable, or inside Docker

Built for real-world datasets, not toy examples.

ğŸ§  Why This Project Exists

Dirty data silently destroys decisions.

This pipeline ensures:

Transparency â†’ You know what was dropped and why

Automation â†’ No manual cleaning

Accountability â†’ Reports + logs + alerts

Scalability â†’ Handles large files consistently

ğŸ› ï¸ Core Features

âœ… CSV & Excel ingestion
âœ… Data validation & cleaning
âœ… Drop-rate analysis & severity scoring
âœ… Excel report generation
âœ… Automated email notifications
âœ… Config-driven thresholds
âœ… Async pipeline execution
âœ… Docker & PyInstaller support
âœ… Logging & audit trail

ğŸ“¦ Deployment Options
Mode	Target User	Description
Manual Run	Analysts / Individuals	Run locally using Python
Standalone EXE	Non-technical users	One-click execution (PyInstaller)
Dockerized	Teams / Enterprises	Consistent, repeatable execution
ğŸ—ï¸ Architecture Overview
data/
 â”œâ”€â”€ raw/            # Input CSV / Excel files
 â”œâ”€â”€ cleaned/        # Cleaned outputs
 â””â”€â”€ reports/        # Analysis reports

src/
 â”œâ”€â”€ main.py         # Pipeline entry point
 â”œâ”€â”€ cleaners/       # Validation & cleaning logic
 â”œâ”€â”€ reporting/      # Excel report generation
 â”œâ”€â”€ notifications/ # Email & alert system
 â””â”€â”€ utils/          # Shared utilities

configs/
 â”œâ”€â”€ client_basic.env
 â”œâ”€â”€ client_email.env
 â””â”€â”€ client_enterprise.env

âš™ï¸ Configuration (Environment-Driven)

All behavior is controlled via .env files:

DATA_FILE=data/raw/input.csv
RECIPIENT_EMAIL=alerts@company.com
DROP_RATE_THRESHOLD=50
INVALID_EMAIL_THRESHOLD=1000
DRY_RUN=false


âœ” No hardcoded client data
âœ” Easy per-client customization
âœ” Safe for production use

â–¶ï¸ Running the Pipeline
Local (Python)
CLIENT_ENV=configs/client_email.env python -m src.main

Docker
docker build -t data-pipeline .
docker run --env-file configs/client_enterprise.env data-pipeline

Standalone (EXE)
Double-click the executable â†’ pipeline runs automatically

ğŸ“§ Email & Alerting

The pipeline automatically sends:

Summary reports

Drop-rate warnings

Data quality alerts

Severity levels:

ğŸŸ¢ LOW

ğŸŸ¡ MEDIUM

ğŸ”´ HIGH

Example log output:

rows=10000 â†’ 3513 | drop_rate=64.87% | severity=HIGH âš ï¸

ğŸ“Š Sample Output

âœ” Cleaned Excel file
âœ” Detailed analysis report
âœ” Logged validation metrics
âœ” Email notification (instant)

ğŸ”’ Reliability & Safety

Defensive validation

Explicit error handling

Logged failures

No silent data loss

Async-safe execution

ğŸ§ª Testing
pytest tests/


Includes:

Pipeline execution tests

Validation behavior checks

Failure-mode handling

ğŸ§° Tech Stack
<div align="center"> <img src="https://skillicons.dev/icons?i=python,docker,github,linux" /> </div>

Python 3

Pandas

AsyncIO

Docker

PyInstaller

SMTP / Email

Pytest

ğŸ“Œ Intended Use Cases

Data cleaning services

Analytics preprocessing

Client data audits

Automated reporting pipelines

Internal data quality monitoring

ğŸ‘¤ Author

Daniel Maina
Aspiring Full-Stack Engineer & Automation Enthusiast
ğŸ“ Nairobi, Kenya

ğŸ“§ Email: kavana.daniel1@gmail.com

ğŸ”— LinkedIn: https://www.linkedin.com/in/daniel-kamau-ab9631389

ğŸ™ GitHub: https://github.com/kamaukavana-dev

ğŸ“„ License

MIT License â€” free to use, modify, and extend.

<div align="center"> <img src="https://capsule-render.vercel.app/api?type=waving&color=gradient&height=120&section=footer&reversal=true" />

Built with discipline, curiosity, and real datasets.

</div>
